{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'speech-based-classification-layer-9/train.csv'\n",
    "valid_path = 'speech-based-classification-layer-9/valid.csv'\n",
    "test_path = 'speech-based-classification-layer-9/test.csv'\n",
    "train = pd.read_csv(train_path)\n",
    "valid = pd.read_csv(valid_path)\n",
    "test = pd.read_csv(test_path)\n",
    "original_train = train.copy()\n",
    "original_valid = train.copy()\n",
    "original_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019301</td>\n",
       "      <td>0.059756</td>\n",
       "      <td>0.081375</td>\n",
       "      <td>0.057481</td>\n",
       "      <td>-0.068440</td>\n",
       "      <td>-0.165913</td>\n",
       "      <td>0.035643</td>\n",
       "      <td>-0.091138</td>\n",
       "      <td>0.021688</td>\n",
       "      <td>0.057158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035576</td>\n",
       "      <td>0.127319</td>\n",
       "      <td>0.098128</td>\n",
       "      <td>-0.058787</td>\n",
       "      <td>0.100971</td>\n",
       "      <td>-0.047754</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049741</td>\n",
       "      <td>0.090030</td>\n",
       "      <td>0.035118</td>\n",
       "      <td>-0.013676</td>\n",
       "      <td>-0.194317</td>\n",
       "      <td>-0.101763</td>\n",
       "      <td>0.085875</td>\n",
       "      <td>-0.081317</td>\n",
       "      <td>0.112418</td>\n",
       "      <td>0.120523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020538</td>\n",
       "      <td>0.058968</td>\n",
       "      <td>0.029803</td>\n",
       "      <td>0.111324</td>\n",
       "      <td>0.036727</td>\n",
       "      <td>0.031927</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019212</td>\n",
       "      <td>0.087779</td>\n",
       "      <td>0.093907</td>\n",
       "      <td>-0.033738</td>\n",
       "      <td>-0.141409</td>\n",
       "      <td>-0.062881</td>\n",
       "      <td>-0.071402</td>\n",
       "      <td>-0.006599</td>\n",
       "      <td>0.020372</td>\n",
       "      <td>-0.027777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119645</td>\n",
       "      <td>-0.040861</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>-0.061003</td>\n",
       "      <td>-0.042450</td>\n",
       "      <td>0.063340</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070283</td>\n",
       "      <td>0.049040</td>\n",
       "      <td>0.042126</td>\n",
       "      <td>0.122637</td>\n",
       "      <td>-0.056964</td>\n",
       "      <td>-0.113700</td>\n",
       "      <td>0.108454</td>\n",
       "      <td>0.051336</td>\n",
       "      <td>0.086610</td>\n",
       "      <td>0.141578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124494</td>\n",
       "      <td>-0.169225</td>\n",
       "      <td>-0.046391</td>\n",
       "      <td>0.148787</td>\n",
       "      <td>0.014616</td>\n",
       "      <td>-0.140644</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.028864</td>\n",
       "      <td>0.165634</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>0.036117</td>\n",
       "      <td>-0.028871</td>\n",
       "      <td>-0.147748</td>\n",
       "      <td>0.053180</td>\n",
       "      <td>0.025071</td>\n",
       "      <td>-0.004200</td>\n",
       "      <td>-0.022183</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124862</td>\n",
       "      <td>0.044907</td>\n",
       "      <td>0.084005</td>\n",
       "      <td>-0.038450</td>\n",
       "      <td>0.084371</td>\n",
       "      <td>-0.072146</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28515</th>\n",
       "      <td>-0.041520</td>\n",
       "      <td>0.302638</td>\n",
       "      <td>0.004811</td>\n",
       "      <td>0.211441</td>\n",
       "      <td>-0.249326</td>\n",
       "      <td>-0.113395</td>\n",
       "      <td>-0.146776</td>\n",
       "      <td>-0.146222</td>\n",
       "      <td>-0.040344</td>\n",
       "      <td>-0.124016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290069</td>\n",
       "      <td>-0.223051</td>\n",
       "      <td>-0.053575</td>\n",
       "      <td>-0.230222</td>\n",
       "      <td>-0.451943</td>\n",
       "      <td>0.117817</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28516</th>\n",
       "      <td>-0.056359</td>\n",
       "      <td>0.061850</td>\n",
       "      <td>0.051865</td>\n",
       "      <td>0.032107</td>\n",
       "      <td>-0.005930</td>\n",
       "      <td>-0.040313</td>\n",
       "      <td>0.117453</td>\n",
       "      <td>-0.067553</td>\n",
       "      <td>0.045004</td>\n",
       "      <td>0.035735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>0.035248</td>\n",
       "      <td>-0.047345</td>\n",
       "      <td>-0.006539</td>\n",
       "      <td>-0.025633</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28517</th>\n",
       "      <td>0.019361</td>\n",
       "      <td>0.148111</td>\n",
       "      <td>0.093434</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>-0.074029</td>\n",
       "      <td>-0.077628</td>\n",
       "      <td>0.049933</td>\n",
       "      <td>-0.096354</td>\n",
       "      <td>0.138556</td>\n",
       "      <td>0.130414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014918</td>\n",
       "      <td>0.015676</td>\n",
       "      <td>-0.033608</td>\n",
       "      <td>0.068212</td>\n",
       "      <td>0.049871</td>\n",
       "      <td>-0.027607</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28518</th>\n",
       "      <td>-0.043644</td>\n",
       "      <td>0.084073</td>\n",
       "      <td>0.074749</td>\n",
       "      <td>0.091776</td>\n",
       "      <td>-0.090782</td>\n",
       "      <td>-0.163187</td>\n",
       "      <td>-0.012568</td>\n",
       "      <td>-0.051021</td>\n",
       "      <td>0.038726</td>\n",
       "      <td>0.011009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098420</td>\n",
       "      <td>0.113067</td>\n",
       "      <td>0.028218</td>\n",
       "      <td>0.036682</td>\n",
       "      <td>0.030056</td>\n",
       "      <td>-0.084346</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28519</th>\n",
       "      <td>-0.039625</td>\n",
       "      <td>0.159638</td>\n",
       "      <td>0.042608</td>\n",
       "      <td>0.046212</td>\n",
       "      <td>-0.025058</td>\n",
       "      <td>-0.143171</td>\n",
       "      <td>0.111391</td>\n",
       "      <td>-0.109869</td>\n",
       "      <td>0.020852</td>\n",
       "      <td>0.152014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>0.111601</td>\n",
       "      <td>-0.065572</td>\n",
       "      <td>0.155703</td>\n",
       "      <td>-0.016872</td>\n",
       "      <td>-0.020940</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28520 rows × 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0       0.019301   0.059756   0.081375   0.057481  -0.068440  -0.165913   \n",
       "1       0.049741   0.090030   0.035118  -0.013676  -0.194317  -0.101763   \n",
       "2       0.019212   0.087779   0.093907  -0.033738  -0.141409  -0.062881   \n",
       "3       0.070283   0.049040   0.042126   0.122637  -0.056964  -0.113700   \n",
       "4       0.028864   0.165634   0.016302   0.036117  -0.028871  -0.147748   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "28515  -0.041520   0.302638   0.004811   0.211441  -0.249326  -0.113395   \n",
       "28516  -0.056359   0.061850   0.051865   0.032107  -0.005930  -0.040313   \n",
       "28517   0.019361   0.148111   0.093434   0.011795  -0.074029  -0.077628   \n",
       "28518  -0.043644   0.084073   0.074749   0.091776  -0.090782  -0.163187   \n",
       "28519  -0.039625   0.159638   0.042608   0.046212  -0.025058  -0.143171   \n",
       "\n",
       "       feature_7  feature_8  feature_9  feature_10  ...  feature_763  \\\n",
       "0       0.035643  -0.091138   0.021688    0.057158  ...    -0.035576   \n",
       "1       0.085875  -0.081317   0.112418    0.120523  ...     0.020538   \n",
       "2      -0.071402  -0.006599   0.020372   -0.027777  ...     0.119645   \n",
       "3       0.108454   0.051336   0.086610    0.141578  ...    -0.124494   \n",
       "4       0.053180   0.025071  -0.004200   -0.022183  ...    -0.124862   \n",
       "...          ...        ...        ...         ...  ...          ...   \n",
       "28515  -0.146776  -0.146222  -0.040344   -0.124016  ...     0.290069   \n",
       "28516   0.117453  -0.067553   0.045004    0.035735  ...     0.004518   \n",
       "28517   0.049933  -0.096354   0.138556    0.130414  ...     0.014918   \n",
       "28518  -0.012568  -0.051021   0.038726    0.011009  ...    -0.098420   \n",
       "28519   0.111391  -0.109869   0.020852    0.152014  ...    -0.000068   \n",
       "\n",
       "       feature_764  feature_765  feature_766  feature_767  feature_768  \\\n",
       "0         0.127319     0.098128    -0.058787     0.100971    -0.047754   \n",
       "1         0.058968     0.029803     0.111324     0.036727     0.031927   \n",
       "2        -0.040861     0.000548    -0.061003    -0.042450     0.063340   \n",
       "3        -0.169225    -0.046391     0.148787     0.014616    -0.140644   \n",
       "4         0.044907     0.084005    -0.038450     0.084371    -0.072146   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "28515    -0.223051    -0.053575    -0.230222    -0.451943     0.117817   \n",
       "28516     0.035248    -0.047345    -0.006539    -0.025633     0.002474   \n",
       "28517     0.015676    -0.033608     0.068212     0.049871    -0.027607   \n",
       "28518     0.113067     0.028218     0.036682     0.030056    -0.084346   \n",
       "28519     0.111601    -0.065572     0.155703    -0.016872    -0.020940   \n",
       "\n",
       "       label_1  label_2  label_3  label_4  \n",
       "0           45       27        1        6  \n",
       "1           45       27        1        6  \n",
       "2           45       27        1        6  \n",
       "3           45       27        1        6  \n",
       "4           45       27        1        6  \n",
       "...        ...      ...      ...      ...  \n",
       "28515       39       29        1        6  \n",
       "28516       39       29        1        6  \n",
       "28517       39       29        1        6  \n",
       "28518       39       29        1        6  \n",
       "28519       39       29        1        6  \n",
       "\n",
       "[28520 rows x 772 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_age = int(train['label_2'].mean())\n",
    "train['label_2'].fillna(mean_age, inplace=True)\n",
    "train['label_2'] = train['label_2'].astype(int)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1       0\n",
       "feature_2       0\n",
       "feature_3       0\n",
       "feature_4       0\n",
       "feature_5       0\n",
       "               ..\n",
       "feature_768     0\n",
       "label_1         0\n",
       "label_2        14\n",
       "label_3         0\n",
       "label_4         0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.130454</td>\n",
       "      <td>0.103857</td>\n",
       "      <td>0.130866</td>\n",
       "      <td>-0.036366</td>\n",
       "      <td>-0.152212</td>\n",
       "      <td>-0.072316</td>\n",
       "      <td>-0.041838</td>\n",
       "      <td>-0.019596</td>\n",
       "      <td>0.038727</td>\n",
       "      <td>0.022496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129246</td>\n",
       "      <td>0.018913</td>\n",
       "      <td>0.013387</td>\n",
       "      <td>-0.054259</td>\n",
       "      <td>0.006282</td>\n",
       "      <td>0.049646</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019179</td>\n",
       "      <td>0.104888</td>\n",
       "      <td>0.079487</td>\n",
       "      <td>0.005187</td>\n",
       "      <td>-0.134111</td>\n",
       "      <td>-0.109271</td>\n",
       "      <td>0.054558</td>\n",
       "      <td>-0.083966</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>0.083015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031823</td>\n",
       "      <td>0.061069</td>\n",
       "      <td>0.036411</td>\n",
       "      <td>0.101357</td>\n",
       "      <td>0.088504</td>\n",
       "      <td>0.008192</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.028485</td>\n",
       "      <td>0.051889</td>\n",
       "      <td>0.033343</td>\n",
       "      <td>-0.029115</td>\n",
       "      <td>-0.091265</td>\n",
       "      <td>-0.038653</td>\n",
       "      <td>0.113460</td>\n",
       "      <td>-0.057983</td>\n",
       "      <td>0.128808</td>\n",
       "      <td>0.062455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033106</td>\n",
       "      <td>-0.007366</td>\n",
       "      <td>-0.027673</td>\n",
       "      <td>0.052090</td>\n",
       "      <td>0.044097</td>\n",
       "      <td>0.054422</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.078959</td>\n",
       "      <td>0.082325</td>\n",
       "      <td>0.068852</td>\n",
       "      <td>-0.028885</td>\n",
       "      <td>-0.146059</td>\n",
       "      <td>-0.071453</td>\n",
       "      <td>-0.028955</td>\n",
       "      <td>0.031253</td>\n",
       "      <td>0.034923</td>\n",
       "      <td>0.019338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164456</td>\n",
       "      <td>-0.025891</td>\n",
       "      <td>0.037993</td>\n",
       "      <td>0.012060</td>\n",
       "      <td>-0.044266</td>\n",
       "      <td>0.045405</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.050968</td>\n",
       "      <td>-0.034524</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.063857</td>\n",
       "      <td>-0.133338</td>\n",
       "      <td>0.102604</td>\n",
       "      <td>-0.031676</td>\n",
       "      <td>0.010629</td>\n",
       "      <td>-0.018684</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102749</td>\n",
       "      <td>0.022433</td>\n",
       "      <td>0.018421</td>\n",
       "      <td>0.086064</td>\n",
       "      <td>0.075351</td>\n",
       "      <td>-0.067089</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>-0.015146</td>\n",
       "      <td>0.075347</td>\n",
       "      <td>-0.067940</td>\n",
       "      <td>0.018324</td>\n",
       "      <td>-0.009130</td>\n",
       "      <td>-0.078199</td>\n",
       "      <td>0.019046</td>\n",
       "      <td>0.054832</td>\n",
       "      <td>0.057062</td>\n",
       "      <td>-0.041307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037656</td>\n",
       "      <td>0.014558</td>\n",
       "      <td>-0.006257</td>\n",
       "      <td>0.114661</td>\n",
       "      <td>-0.020831</td>\n",
       "      <td>0.043870</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>-0.093367</td>\n",
       "      <td>0.090779</td>\n",
       "      <td>-0.014720</td>\n",
       "      <td>0.060278</td>\n",
       "      <td>-0.054997</td>\n",
       "      <td>0.012507</td>\n",
       "      <td>0.138347</td>\n",
       "      <td>0.064434</td>\n",
       "      <td>0.021669</td>\n",
       "      <td>0.038423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102469</td>\n",
       "      <td>-0.012509</td>\n",
       "      <td>-0.024472</td>\n",
       "      <td>-0.082420</td>\n",
       "      <td>-0.018768</td>\n",
       "      <td>-0.091912</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>0.036313</td>\n",
       "      <td>0.053569</td>\n",
       "      <td>-0.040507</td>\n",
       "      <td>0.084671</td>\n",
       "      <td>-0.047545</td>\n",
       "      <td>-0.127989</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.038251</td>\n",
       "      <td>0.058703</td>\n",
       "      <td>0.012898</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.139285</td>\n",
       "      <td>-0.020544</td>\n",
       "      <td>-0.086482</td>\n",
       "      <td>0.202496</td>\n",
       "      <td>-0.089369</td>\n",
       "      <td>-0.052960</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>0.002803</td>\n",
       "      <td>0.127188</td>\n",
       "      <td>0.091111</td>\n",
       "      <td>-0.024576</td>\n",
       "      <td>-0.044046</td>\n",
       "      <td>-0.065907</td>\n",
       "      <td>-0.019063</td>\n",
       "      <td>-0.062951</td>\n",
       "      <td>0.066077</td>\n",
       "      <td>0.058489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055279</td>\n",
       "      <td>-0.014534</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>0.017866</td>\n",
       "      <td>-0.008768</td>\n",
       "      <td>-0.017424</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>0.007929</td>\n",
       "      <td>0.087711</td>\n",
       "      <td>0.077855</td>\n",
       "      <td>0.104332</td>\n",
       "      <td>-0.095509</td>\n",
       "      <td>-0.124483</td>\n",
       "      <td>-0.045992</td>\n",
       "      <td>-0.021891</td>\n",
       "      <td>0.083542</td>\n",
       "      <td>-0.006199</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075926</td>\n",
       "      <td>0.096852</td>\n",
       "      <td>0.014226</td>\n",
       "      <td>-0.003264</td>\n",
       "      <td>0.047408</td>\n",
       "      <td>0.016309</td>\n",
       "      <td>39</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750 rows × 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0    -0.130454   0.103857   0.130866  -0.036366  -0.152212  -0.072316   \n",
       "1     0.019179   0.104888   0.079487   0.005187  -0.134111  -0.109271   \n",
       "2    -0.028485   0.051889   0.033343  -0.029115  -0.091265  -0.038653   \n",
       "3    -0.078959   0.082325   0.068852  -0.028885  -0.146059  -0.071453   \n",
       "4     0.001748   0.050968  -0.034524   0.074153   0.063857  -0.133338   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "745  -0.015146   0.075347  -0.067940   0.018324  -0.009130  -0.078199   \n",
       "746  -0.093367   0.090779  -0.014720   0.060278  -0.054997   0.012507   \n",
       "747   0.036313   0.053569  -0.040507   0.084671  -0.047545  -0.127989   \n",
       "748   0.002803   0.127188   0.091111  -0.024576  -0.044046  -0.065907   \n",
       "749   0.007929   0.087711   0.077855   0.104332  -0.095509  -0.124483   \n",
       "\n",
       "     feature_7  feature_8  feature_9  feature_10  ...  feature_763  \\\n",
       "0    -0.041838  -0.019596   0.038727    0.022496  ...     0.129246   \n",
       "1     0.054558  -0.083966  -0.000091    0.083015  ...    -0.031823   \n",
       "2     0.113460  -0.057983   0.128808    0.062455  ...    -0.033106   \n",
       "3    -0.028955   0.031253   0.034923    0.019338  ...     0.164456   \n",
       "4     0.102604  -0.031676   0.010629   -0.018684  ...    -0.102749   \n",
       "..         ...        ...        ...         ...  ...          ...   \n",
       "745   0.019046   0.054832   0.057062   -0.041307  ...    -0.037656   \n",
       "746   0.138347   0.064434   0.021669    0.038423  ...    -0.102469   \n",
       "747   0.048077   0.038251   0.058703    0.012898  ...    -0.139285   \n",
       "748  -0.019063  -0.062951   0.066077    0.058489  ...     0.055279   \n",
       "749  -0.045992  -0.021891   0.083542   -0.006199  ...    -0.075926   \n",
       "\n",
       "     feature_764  feature_765  feature_766  feature_767  feature_768  label_1  \\\n",
       "0       0.018913     0.013387    -0.054259     0.006282     0.049646       45   \n",
       "1       0.061069     0.036411     0.101357     0.088504     0.008192       45   \n",
       "2      -0.007366    -0.027673     0.052090     0.044097     0.054422       45   \n",
       "3      -0.025891     0.037993     0.012060    -0.044266     0.045405       45   \n",
       "4       0.022433     0.018421     0.086064     0.075351    -0.067089       45   \n",
       "..           ...          ...          ...          ...          ...      ...   \n",
       "745     0.014558    -0.006257     0.114661    -0.020831     0.043870       39   \n",
       "746    -0.012509    -0.024472    -0.082420    -0.018768    -0.091912       39   \n",
       "747    -0.020544    -0.086482     0.202496    -0.089369    -0.052960       39   \n",
       "748    -0.014534     0.014749     0.017866    -0.008768    -0.017424       39   \n",
       "749     0.096852     0.014226    -0.003264     0.047408     0.016309       39   \n",
       "\n",
       "     label_2  label_3  label_4  \n",
       "0         27        1        6  \n",
       "1         27        1        6  \n",
       "2         27        1        6  \n",
       "3         27        1        6  \n",
       "4         27        1        6  \n",
       "..       ...      ...      ...  \n",
       "745       29        1        6  \n",
       "746       29        1        6  \n",
       "747       29        1        6  \n",
       "748       29        1        6  \n",
       "749       29        1        6  \n",
       "\n",
       "[750 rows x 772 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_age = int(train['label_2'].mean())\n",
    "valid['label_2'].fillna(mean_age, inplace=True)\n",
    "valid['label_2'] = valid['label_2'].astype(int)\n",
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      0\n",
       "feature_2      0\n",
       "feature_3      0\n",
       "feature_4      0\n",
       "feature_5      0\n",
       "              ..\n",
       "feature_768    0\n",
       "label_1        0\n",
       "label_2        0\n",
       "label_3        0\n",
       "label_4        0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label 2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train.iloc[:, :768]\n",
    "train_label_2 = train.iloc[:, 769]\n",
    "\n",
    "valid_features = valid.iloc[:, :768]\n",
    "valid_label_2 = valid.iloc[:, 769]\n",
    "\n",
    "test_features = test.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_counts = train_label_2.value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "class_counts.plot(kind='bar', color='skyblue')\n",
    "\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the best model from random forest model and svc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "}\n",
    "\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    total_accuracy = 0.0\n",
    "\n",
    "    for train_index, test_index in skf.split(train_features, train_label_2):\n",
    "        X_train, X_test = train_features.iloc[train_index], train_features.iloc[test_index]\n",
    "        y_train, y_test = train_label_2[train_index], train_label_2[test_index]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        total_accuracy += accuracy\n",
    "\n",
    "    average_accuracy = total_accuracy / n_splits\n",
    "    print(f\"{name} - Average Accuracy: {average_accuracy}\")\n",
    "\n",
    "\n",
    "    if average_accuracy > best_accuracy:\n",
    "        best_model = clf\n",
    "        best_accuracy = average_accuracy\n",
    "\n",
    "print(f\"Best Model: {type(best_model)._name_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "transformer = RobustScaler()\n",
    "scaled_train_features = transformer.fit_transform(train_features)\n",
    "scaled_valid_features = transformer.fit_transform(valid_features)\n",
    "scaled_test_features = transformer.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE For class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42 )\n",
    "train_feature_resampled, train_label_3_resampled = smote.fit_resample(scaled_train_features, train_label_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def performPca(train_input, valid_input,test_input, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    train_reduced = pca.fit_transform(train_input)\n",
    "    valid_reduced = pca.transform(valid_input)\n",
    "    test_reduced = pca.transform(test_input)\n",
    "    train_reduced_df = pd.DataFrame(train_reduced, columns=[f\"new_feature_{i+1}\" for i in range(train_reduced.shape[1])])\n",
    "    valid_reduced_df = pd.DataFrame(valid_reduced, columns=[f\"new_feature_{i+1}\" for i in range(valid_reduced.shape[1])])\n",
    "    test_reduced_df = pd.DataFrame(test_reduced, columns=[f\"new_feature_{i+1}\" for i in range(test_reduced.shape[1])])\n",
    "\n",
    "\n",
    "    return train_reduced_df, valid_reduced_df,test_reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reduced_df, valid_reduced_df,test_reduced_df = performPca(train_feature_resampled, scaled_valid_features, scaled_test_features, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.936\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "model.fit(train_reduced_df, train_label_3_resampled)\n",
    "y_pred = model.predict(valid_reduced_df)\n",
    "accuracy = accuracy_score(valid_label_2, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.912\n",
    "test_pred = model.predict(test_reduced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to file\n",
    "test_pred_df = pd.DataFrame(test_pred, columns=['label_2'])\n",
    "test_pred_df.to_csv('predictions/label_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV 1/3] END ........C=0.1, gamma=1, kernel=rbf;, score=0.167 total time= 9.0min\n",
      "[CV 2/3] END ........C=0.1, gamma=1, kernel=rbf;, score=0.167 total time=12.5min\n",
      "[CV 3/3] END ........C=0.1, gamma=1, kernel=rbf;, score=0.167 total time=13.9min\n",
      "[CV 1/3] END .....C=0.1, gamma=1, kernel=linear;, score=0.300 total time= 2.5min\n",
      "[CV 2/3] END .....C=0.1, gamma=1, kernel=linear;, score=0.425 total time= 2.7min\n",
      "[CV 3/3] END .....C=0.1, gamma=1, kernel=linear;, score=0.306 total time= 2.6min\n",
      "[CV 1/3] END .......C=0.1, gamma=1, kernel=poly;, score=0.318 total time= 5.5min\n",
      "[CV 2/3] END .......C=0.1, gamma=1, kernel=poly;, score=0.489 total time= 5.9min\n",
      "[CV 3/3] END .......C=0.1, gamma=1, kernel=poly;, score=0.326 total time= 5.5min\n",
      "[CV 1/3] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.167 total time= 8.1min\n",
      "[CV 2/3] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.167 total time= 6.8min\n",
      "[CV 3/3] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.167 total time= 7.8min\n",
      "[CV 1/3] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.300 total time= 1.6min\n",
      "[CV 2/3] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.425 total time= 1.6min\n",
      "[CV 3/3] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.306 total time= 1.7min\n",
      "[CV 1/3] END .....C=0.1, gamma=0.1, kernel=poly;, score=0.318 total time= 3.4min\n",
      "[CV 2/3] END .....C=0.1, gamma=0.1, kernel=poly;, score=0.489 total time= 3.5min\n",
      "[CV 3/3] END .....C=0.1, gamma=0.1, kernel=poly;, score=0.325 total time= 3.4min\n",
      "[CV 1/3] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.167 total time= 9.3min\n",
      "[CV 2/3] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.167 total time= 6.5min\n",
      "[CV 3/3] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.167 total time= 6.2min\n",
      "[CV 1/3] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.300 total time= 1.6min\n",
      "[CV 2/3] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.425 total time= 1.7min\n",
      "[CV 3/3] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.306 total time= 1.7min\n",
      "[CV 1/3] END ....C=0.1, gamma=0.01, kernel=poly;, score=0.320 total time= 3.6min\n",
      "[CV 2/3] END ....C=0.1, gamma=0.01, kernel=poly;, score=0.490 total time= 3.7min\n",
      "[CV 3/3] END ....C=0.1, gamma=0.01, kernel=poly;, score=0.324 total time= 3.5min\n",
      "[CV 1/3] END ..........C=1, gamma=1, kernel=rbf;, score=0.167 total time=11.6min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1], \n",
    "              'gamma': [1, 0.1, 0.01],\n",
    "              'kernel': ['rbf' , 'linear', 'poly']} \n",
    "  \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3 , cv=3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "grid.fit(train_reduced_df, train_label_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "fin_model = SVC(C= 0.1, gamma = 1, kernel = 'linear')\n",
    "fin_model.fit(train_reduced_df, train_label_3_resampled)\n",
    "fin_y_pred = fin_model.predict(valid_reduced_df)\n",
    "accuracy = accuracy_score(valid_label_2, fin_y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_test_pred = fin_model.predict(test_reduced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to file\n",
    "fin_test_pred_df = pd.DataFrame(fin_test_pred, columns=['label_1'])\n",
    "fin_test_pred_df.to_csv('predictions/label_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
